{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we go through the work done for our data cleaning.  This is being done on the data that was actually used, with exteme emperical oversampling of the target class.  Aside from insights originating from direct sporadic observation of the data, however, the procedures are in principle blind to the overall picture of the data and could be applied to other reddit comments used to supplement this data.\n",
    "\n",
    "The procedures done here are also located in the <font color='red'>put package here</font> subpackage of the <font color='red'>module</font>, along with a table of the regex expressions used and their intent.  Our goal here is to normalize our text for both the purpose of analysis but also to make using a larger sample more tractable for global learning and analysis on a single machine.  The resulting file, used for model training, is already located under the `data_sets` directory, this notebook shows how it is created but does not *need* to be ran for the other notebooks to work. \n",
    "\n",
    "We begin by importing the \"crude working data\" created in the <font color='green'>data harvesting</font> notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!conda install pyarrow\n",
    "#Take this out if it doesn't work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer \n",
    "\n",
    "\n",
    "working_data=pd.read_csv(r'..\\data_sets\\crude_data_enc_utf8.csv',names=['snapshot','subreddit','label'],\n",
    "                        sep='|',encoding='utf-8',header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ArrowIOError",
     "evalue": "Invalid parquet file. Corrupt footer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowIOError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-a91357eca100>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'..\\data_sets\\crude_data_parq.parquet'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\final_project_env\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, **kwargs)\u001b[0m\n\u001b[0;32m    315\u001b[0m     \"\"\"\n\u001b[0;32m    316\u001b[0m     \u001b[0mimpl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mimpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\final_project_env\\lib\\site-packages\\pandas\\io\\parquet.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, path, columns, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"use_pandas_metadata\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m         result = self.api.parquet.read_table(\n\u001b[1;32m--> 142\u001b[1;33m             \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m         ).to_pandas()\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\final_project_env\\lib\\site-packages\\pyarrow\\parquet.py\u001b[0m in \u001b[0;36mread_table\u001b[1;34m(source, columns, use_threads, metadata, use_pandas_metadata, memory_map, read_dictionary, filesystem, filters, buffer_size)\u001b[0m\n\u001b[0;32m   1272\u001b[0m                             \u001b[0mread_dictionary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mread_dictionary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m                             \u001b[0mbuffer_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1274\u001b[1;33m                             filesystem=filesystem, filters=filters)\n\u001b[0m\u001b[0;32m   1275\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m         pf = ParquetFile(source, metadata=metadata,\n",
      "\u001b[1;32m~\\.conda\\envs\\final_project_env\\lib\\site-packages\\pyarrow\\parquet.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_or_paths, filesystem, schema, metadata, split_row_groups, validate_schema, filters, metadata_nthreads, read_dictionary, memory_map, buffer_size)\u001b[0m\n\u001b[0;32m   1058\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1059\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalidate_schema\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1060\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_schemas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1062\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\final_project_env\\lib\\site-packages\\pyarrow\\parquet.py\u001b[0m in \u001b[0;36mvalidate_schemas\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1090\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1092\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpieces\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\final_project_env\\lib\\site-packages\\pyarrow\\parquet.py\u001b[0m in \u001b[0;36mget_metadata\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \u001b[0mmetadata\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mFileMetaData\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m         \"\"\"\n\u001b[1;32m--> 560\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    561\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\final_project_env\\lib\\site-packages\\pyarrow\\parquet.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    565\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mParquetFile\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m         \"\"\"\n\u001b[1;32m--> 567\u001b[1;33m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_file_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    568\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParquetFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m             \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mParquetFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\final_project_env\\lib\\site-packages\\pyarrow\\parquet.py\u001b[0m in \u001b[0;36m_open_dataset_file\u001b[1;34m(dataset, path, meta)\u001b[0m\n\u001b[0;32m    940\u001b[0m         \u001b[0mread_dictionary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_dictionary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m         \u001b[0mcommon_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon_metadata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m         \u001b[0mbuffer_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     )\n\u001b[0;32m    944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\final_project_env\\lib\\site-packages\\pyarrow\\parquet.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, source, metadata, common_metadata, read_dictionary, memory_map, buffer_size)\u001b[0m\n\u001b[0;32m    135\u001b[0m         self.reader.open(source, use_memory_map=memory_map,\n\u001b[0;32m    136\u001b[0m                          \u001b[0mbuffer_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m                          read_dictionary=read_dictionary, metadata=metadata)\n\u001b[0m\u001b[0;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommon_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcommon_metadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nested_paths_by_prefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_nested_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\final_project_env\\lib\\site-packages\\pyarrow\\_parquet.pyx\u001b[0m in \u001b[0;36mpyarrow._parquet.ParquetReader.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\final_project_env\\lib\\site-packages\\pyarrow\\error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowIOError\u001b[0m: Invalid parquet file. Corrupt footer."
     ]
    }
   ],
   "source": [
    "pd.read_parquet(r'..\\data_sets\\crude_data_parq.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing stil failing, christ I don't get how everything that can go wrong is going wrong. I\n",
    "#'ll just procede as though the data is good and hopefully won't forget to undo the drop. \n",
    "# Y O U   N E E \n",
    "# D\n",
    "# D\n",
    "\n",
    "working_data.dropna(inplace=True)\n",
    "\n",
    "# TO\n",
    "# DLETE\n",
    "# THIS\n",
    "# CELL\n",
    "\n",
    "t=pd.read_csv(r'..\\crude_data_pipe_sep.csv',header=None,names=['snapshot','subreddit','label'],sep='|',encoding='UTF-8')\n",
    "\n",
    "\n",
    "#better but still bad somehow.  How is it making a mistake still? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first take everything to lowercase and then try to drop anything problematic.  There are two types of meaningful tokens we want to be aware of besides works: the first, website urls-- we want to preserve this information the best that we can, at least initially (consider that choosing to drop websites will be as hard to do evenly across the board as finding a way to condense them).  The second is internal reddit username and subforum links, which are written as /u/user_name or /r/subreddit_name.  Nominally the links should begin with a /, though investigation shows that often is not the case.  The name itself can consist of a sequence of letters, numbers, or underscores.  We'd like to preserve this structure (we don't want to confuse \"/r/politics\", a political board on reddit, with the normal term \"politics\" for instance).  We'll write further text processing with this in mind, for now though we'll pass a regex substitution to normalize the expressions:\n",
    "\n",
    "We will match anything of the form /u/valid_name_format or /r/valid_name_format and drop the leading /. These features, which are advisable to complete prior to the next step, can be accessed independently by the `basic_reddit_preformatting` method of `ex_id_tools.text_cleaning`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data['snapshot']=working_data['snapshot'].str.lower()\n",
    "#send all words to lowercase\n",
    "working_data['snapshot']=working_data.snapshot.str.replace('(?<![^\\s])/(?=\\w/[\\w]+)',' ')\n",
    "#clean /x/yz-type strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>output</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then filter out non-english subforums; nominally part of data cleaning but not viable in an advisable way before basic text cleaning.  This is was done by identifying the ratio of non-english stopwords to english stopwords in each entry, then sorting the data by that ratio to try to find subreddits that may not be english-based.  This is a quite crude method, but allows for computational feasibility that more sophisticated means (using a pretrained classifier, presumably, or using a larger lexicon) would lack. \n",
    "\n",
    "The functionality is given in the `text_cleaning` submodule, under the name `crude_language_detect_toll`.  This series is used to define a sorted dataframe, then repeated slices can be displayed to find non-english subreddits.  The list of those found for this data set is saved as `foreign_sub_list.p` in the `data_sets` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>snapshot</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wtf  one guess where i live incredibly aggrava...</td>\n",
       "      <td>0XPROJECT</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>lol meant to reference the guy who deleted his...</td>\n",
       "      <td>1819</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>sounds fun as hell i have just the guardian fa...</td>\n",
       "      <td>2B2T</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>thats what im leaning toward as well but a sea...</td>\n",
       "      <td>3DPRINTING</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>no problem and id recommend both if you arent ...</td>\n",
       "      <td>3DPRINTING</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20998</th>\n",
       "      <td>chapotrashhouse is this regarding my inability...</td>\n",
       "      <td>NEOLIBERAL</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20999</th>\n",
       "      <td>this whole russia thing is like a more deprave...</td>\n",
       "      <td>NEOLIBERAL</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21000</th>\n",
       "      <td>the new zealand national party released an ad ...</td>\n",
       "      <td>NEOLIBERAL</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21001</th>\n",
       "      <td>source on the 80 http://wwwnberorg/papers/w149...</td>\n",
       "      <td>NEOLIBERAL</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21002</th>\n",
       "      <td>im sorry to hear that youre free to talk about...</td>\n",
       "      <td>NEOLIBERAL</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17950 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                snapshot   subreddit  label\n",
       "0      wtf  one guess where i live incredibly aggrava...   0XPROJECT    0.0\n",
       "4      lol meant to reference the guy who deleted his...        1819    0.0\n",
       "33     sounds fun as hell i have just the guardian fa...        2B2T    0.0\n",
       "38     thats what im leaning toward as well but a sea...  3DPRINTING    0.0\n",
       "39     no problem and id recommend both if you arent ...  3DPRINTING    0.0\n",
       "...                                                  ...         ...    ...\n",
       "20998  chapotrashhouse is this regarding my inability...  NEOLIBERAL    1.0\n",
       "20999  this whole russia thing is like a more deprave...  NEOLIBERAL    1.0\n",
       "21000  the new zealand national party released an ad ...  NEOLIBERAL    1.0\n",
       "21001  source on the 80 http://wwwnberorg/papers/w149...  NEOLIBERAL    1.0\n",
       "21002  im sorry to hear that youre free to talk about...  NEOLIBERAL    1.0\n",
       "\n",
       "[17950 rows x 3 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foreign_list=pickle.load(open(r'..\\data_sets\\foreign_sub_list.p','rb'))\n",
    "working_data.drop(working_data[working_data['subreddit'].isin(foreign_list)].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_tokenizer=RegexpTokenizer(r'http[s]*[:]*[\\w_/]+|www[\\w/_]+|[ur]/[\\w]+|[a-z]{2,}')\n",
    "\n",
    "working_data['snapshot']=working_data['snapshot'].map(reddit_tokenizer.tokenize)\n",
    "working_data['snapshot']=working_data['snapshot'].map(lambda l: [s for s in l if s not in stopwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, initial modelling was attempted.  The results stimulated further cleaning, which is then done below.  First, we do take out the websites we previously had chosen to keep.  Note that in the filtering & tokenizing we're running the risk of losing some information, particularly non-english top-level domains (co.uk, now as co uk, begins to float around, for instance).  This helps motivate aggressive feature dropping later on-- already likely a good idea-- to try to suppress the effects of both the already existing noise and the incidental noise we introduce in trying to better isolate good features.  \n",
    "\n",
    "We should be aware at this point we are going to lose some messages written in a not-unpopular fromat <font color='gold'>O F T Y P I N G L I K E T H I S</font>\n",
    "\n",
    "We then attempt to collapse down repetitions, as users will often type \"hahahahahahahahahahahaha, hahahahahahahaha\", etc and those should likely be treated as the same.  Having handled this, we can then safely drop words that are of length greater than 45, which were read in incorrectly at some point or are otherwise unwanted, such as long internal/diagnostic text or websites.  Note this choice is for the purposes of computability and expedience; a preferable method in the absence of those would involve at least some information recovery from those strings with readily exploitable opportunities to be split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-61fe06394250>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m working_data['snapshot']=(working_data['snapshot']).map(lambda s:\n\u001b[0m\u001b[0;32m      2\u001b[0m                         [word for word in s if not re.match('http[s]*[:]*[\\w_/]+|www[\\w/_]+',word)])\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#\"url_start\" in .text_cleaning.regex_store\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m working_data['snapshot']=working_data['snapshot'].map(    \n\u001b[0;32m      5\u001b[0m         lambda s: list(\n",
      "\u001b[1;32m~\\.conda\\envs\\final_project_env\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   3968\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3969\u001b[0m         \"\"\"\n\u001b[1;32m-> 3970\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3971\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[0;32m   3972\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"map\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\final_project_env\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1160\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-61fe06394250>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      1\u001b[0m working_data['snapshot']=(working_data['snapshot']).map(lambda s:\n\u001b[1;32m----> 2\u001b[1;33m                         [word for word in s if not re.match('http[s]*[:]*[\\w_/]+|www[\\w/_]+',word)])\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#\"url_start\" in .text_cleaning.regex_store\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m working_data['snapshot']=working_data['snapshot'].map(    \n\u001b[0;32m      5\u001b[0m         lambda s: list(\n",
      "\u001b[1;32m<ipython-input-26-61fe06394250>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m working_data['snapshot']=(working_data['snapshot']).map(lambda s:\n\u001b[1;32m----> 2\u001b[1;33m                         [word for word in s if not re.match('http[s]*[:]*[\\w_/]+|www[\\w/_]+',word)])\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#\"url_start\" in .text_cleaning.regex_store\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m working_data['snapshot']=working_data['snapshot'].map(    \n\u001b[0;32m      5\u001b[0m         lambda s: list(\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "working_data['snapshot']=(working_data['snapshot']).map(lambda s:\n",
    "                        [word for word in s if not re.match('http[s]*[:]*[\\w_/]+|www[\\w/_]+',word)])\n",
    "#\"url_start\" in .text_cleaning.regex_store\n",
    "working_data['snapshot']=working_data['snapshot'].map(    \n",
    "        lambda s: list(\n",
    "            map(\n",
    "            (lambda w: re.sub(r'(\\w{1,3}\\w{0,3})\\1{3,}','\\g<1>\\g<1>',w))\n",
    "            ,s)))\n",
    "#\"repetitions_in and reptitions_out\" in .text_cleaning.regex_store.  This will consense 1-and-2 letter repetitions\n",
    "\n",
    "working_data['snapshot']=working_data['snapshot'].apply(lambda r: [w for w in r if len(w)<=45])\n",
    "#Drop long strings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then join our strings back together and resume further filtering, again informed by the results of intermediate models, on the data remaining.  First, we'll handle the abundant formatting characters like 'nbsp' that appear.  \n",
    "We'll use the list of character entity reference from:https://www.w3schools.com/html/html_entities.asp .  For ease we'll rejoin the snapshot column before working. In dropping these we take special care with respect to the details of the english language, a word ending with \"nbsp' probably just has that on as a mistake, but a word ending with \"amp\" is likely just a normal english word.  An opportunity exists here for further filtering by examining the distribution of, say, words ending in -amp and the five words that occur on either side, to see what can and cannot be safely extroplated relative to the exact data being examined.  This was done to some extent here, and further informs the regex used-- the word boundaries are such that, wherever they are not present, they are such that *no* words are mistkenly dropped.  They are quite conservative as a result. \n",
    "\n",
    "We first drop extract the center for anything of the form likely intended to be of the form <font color='blue'>**\\<**</font>\n",
    "    text\n",
    "    <font color='blue'>**\\>**\n",
    "        </font>, which would be possibly mis-represented in our data as <font color='blue'>**gt**</font>text<font color='blue'>**lt**</font>.  We then drop expected hmtl entities.  We will be introducing errant vocabulary here, expecially for simple hmtl tags; most are short and will be dropped or should be eliminated through feature extraction but it will be something to pay attention to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_entities=r'|'.join([r'[\\w]*nbsp[\\w]*',r'\\blt[\\w]*',r'\\bgt[\\w]*',r'\\bamp\\b',r'\\bquot\\b',r'\\bpos\\b'])\n",
    "#html_entities in  .text_cleaning.regex_store\n",
    "working_data['snapshot']=working_data['snapshot'].str.replace(r'lt(\\w*)gt',r'\\g<1>')\n",
    "#'html_tag_like_in' and 'html_tag_like_out' in text_cleaning.regex_store\n",
    "\n",
    "working_data['snapshot']=working_data['snapshot'].str.replace(html_entities,'')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some likely website addresses still remain; further, errant http or https strings were found at the end of innocuous words; in general this should be handled with careful grouping and splitting to preserve what follows -- and the complexity of the first regex pattern used expects & reflects that, but investigation showed here that we can just drop said patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_data['snapshot']=working_data['snapshot'].str.replace(r'\\b\\w*http(?:[^s\\s]|(?:s\\w))+\\w*\\b','')\n",
    "#likely_url in regex-store\n",
    "working_data['snapshot']=working_data['snapshot'].str.replace(r'(\\w)http[s]*',r'\\g<1>')\n",
    "#'http_end_in' and 'http_end_out' in regex_store\n",
    "\n",
    "working_data['snapshot']=working_data['snapshot'].str.replace(r'\\w+(?:png|gif|jpg|jpeg)\\w*','')\n",
    "#image_endings in regex_store\n",
    "#drop likely image links, note this is a little more information-compromising\n",
    "#than might be expected due to the popularity of written out expressions like 'sad.jpg'\n",
    "\n",
    "working_data['snapshot']=working_data['snapshot'].str.replace(r'\\bcom\\b','')\n",
    "#isoloted_com in regex_store\n",
    "\n",
    "working_data['snapshot']=working_data['snapshot'].str.replace(r'\\bwww\\b','')\n",
    "#isolated_www in regex_store\n",
    "\n",
    "working_data['snapshot']=working_data['snapshot'].str.replace(r'http[s]*(?=\\s)','')\n",
    "working_data['snapshot']=working_data['snapshot'].str.replace(r'(?<=\\s)http[s]*','')\n",
    "#strings starting or ending with http"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More cleaning can likely be done.  Particularly, phrases like \"submission removed\" appear repatedly and represent some automatic action being taken that showed up in the post contents.  As we see elsewhere, though, we are able to get a good model with reasonable features from what results here. \n",
    "\n",
    "As promised, we can create our desired data by just running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ex_id_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-01bb261054ee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mex_id_tools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtext_cleaning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtext_cleaning\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclean_series\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworking_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mforeign_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'..\\data_sets\\foreign_sub_list.p'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ex_id_tools'"
     ]
    }
   ],
   "source": [
    "from ex_id_tools import text_cleaning\n",
    "text_cleaning.clean_series(working_data,foreign_list=pickle.load(open(r'..\\data_sets\\foreign_sub_list.p','rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
